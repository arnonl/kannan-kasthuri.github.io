---
# title: "Lecture 02"
# author: "Kasthuri Kannan"
# date: "Sept 07, 2017"
output: html_document
# output: slidy_presentation

---

### Algorithms (Sept. 20, 2017)

---

This lecture is designed to give a bird's eye view of analysis of algorithms. The subject is extremely vast and it is usually a semester long course (typically a core course) in computer science departments. 

##### Motivation

Computer programs are set of instructions, or algorithm, supplied by the programmer to accomplish a certain task. There can be several algorithms that can accomplish the same computational task. For instance, consider sorting a bunch of cards. This could be done at least in two different ways, namely, by insertion sort or through selection sort -

---

<center>![](insertion_selection_sort.png)</center>

---

Which is a "better" sort, in terms of time, as the number of cards increase? The word "better" here is often referred as _computational complexity_ in the context of analysis of algorithms. 

Fundemental to analysis of algorithms is asymptotic analysis which helps in determining the computational complexity of such algorithms. The computational (time) complexity of an algorithm is something that decides how fast the algorithm is run at the idea level (otherwise known as pseudocode), regardless of the programming language, operating system or the hardware. We want to evaluate and compare algorithms as they are: an actionable/programmable idea on how things could be computed. In other words, we are **not interested** in how fast a program is actually run, which depends on various factors including the structure of high-level languages, but rather how best (or worst) is the design behind the program, regardless of the language it is implemented (by analogy, a person could sort faster or slower and we are not interested in the speed someone sorts). Such meta-analysis enables us to say an algorithm has a better computational complexity than the other.

Aymptotic analysis also allows us to explain how an algorithm would behave as the input size become large, or if the distribution of the input changes. For example, in the case of insertion or selection sort, we can _predict_ how the algorithm would perform if we give numbers that are already sorted vs. random unsorted numbers. Such predictions are important in practical programming since we can determine the scalability of operations.    

<br>

#### Counting Instructions 



---

#### An Introduction to AoA - Sorting


##### Selection Sort


##### Insertion Sort


##### Divide and Conquer 


---

#### Dynamic Programming 


##### Assembly Line Problem


##### Finding DNA Subsequence


---

#### Greedy Algorithms


---

#### An Introduction to NP-completeness


---

#### References

[A Gentle Introduction to Algorithm Complexity Analysis](http://discrete.gr/complexity/) by Dionysis "dionyziz" Zindros


