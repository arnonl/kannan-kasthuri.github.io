---
# title: "Lecture 02"
# author: "Kasthuri Kannan"
# date: "Sept 07, 2017"
output: html_document
# output: slidy_presentation

---

### Algorithms (Sept. 20, 2017)

---

This lecture is designed to give a bird's eye view of analysis of algorithms. The subject is extremely vast and it is usually a semester long course (typically a core course) in computer science departments. 

#### Motivation

Computer programs are set of instructions, or algorithm, supplied by the programmer to accomplish a certain task. There can be several algorithms that can accomplish the same computational task. For instance, consider sorting a bunch of cards. This could be done at least in two different ways, namely, by insertion sort or through selection sort -

---

<center>![](insertion_selection_sort.png)</center>

---

Which is a "better" sort, in terms of time, as the number of cards increase? The word "better" here is often referred as _computational complexity_ in the context of analysis of algorithms. 

Fundemental to analysis of algorithms is complexity analysis which helps in determining the computational complexity of such algorithms. The computational (time) complexity of an algorithm is something that decides how fast the algorithm is run at the idea level (otherwise known as pseudocode), regardless of the programming language, operating system or the hardware. We want to evaluate and compare algorithms as they are: an actionable/programmable idea on how things could be computed. In other words, we are **not interested** in how fast a program is actually run, which depends on various factors including the structure of high-level languages, but rather how best (or worst) is the design behind the program, regardless of the language it is implemented (by analogy, a person could sort faster or slower and we are not interested in the speed someone sorts). Such meta-analysis enables us to say an algorithm has a better computational complexity than the other.

Complexity analysis also allows us to explain how an algorithm would behave as the input size become large, or if the distribution of the input changes. For example, in the case of insertion or selection sort, we can _predict_ how the algorithm would perform if we give numbers that are already sorted vs. random unsorted numbers. Such predictions are important in practical programming since we can determine the scalability of operations assuming worst-case/best-case scenario.    

---

#### Counting Instructions 

Computational complexity is usually determined by counting instructions. Let's find the complexity for the following computational task.

**Task** Detrmine the minimum element in an array $A$ with $n$ elements addressed by $A[1], A[2],\dots, A[n]$.

**Algorithm** The pesudocode to do this task is as follows:

```{r eval=FALSE}

  # Assign M to the first element in the array
  M = A[1]
  # Go through the array and if the number in the array is less than M, then update M with that number
  for i = 2 to n, increment i
    if (A[i] is less than or equal to M) then
        replace M with A[i]
    end if
  end for
  # Print the answer
  print M is the smallest number
    
```

To work out the complexity we will assume that our hypothetical machine can perform the following operations as one instruction each:

* Assigning a value to a variable
* Looking up the value of a particular element in an array
* Comparing two values
* Incrementing a value
* Basic arithmetic operations such as addition and multiplication

Thus, the first assignment statement:

```{r eval=FALSE}
  M = A[1]
```

will require $2$ instructions, one for looking up $A[1]$ and the other for assigning $A[1]$ to $M$. During the first time of the `for` loop, $i$ has to be assigned $2$ and the condition $2 \leq n$ needs to be evaluated, which will take two instuctions. After this, incrementing $i$ and checking if $i \leq n$ has to be done $n-1$ times, contributing $2(n-1)$ instructions.

We can now define a mathematical function $g(n)$ that, given an $n$, gives us the number of instructions the algorithm needs. Thus, if we ignore the `for` loop body (for the moment), the number of instructions would be $g(n)=2+2+2(n-1)=2n+2$.

---

#### Worst-case complexity

Let's look into the `for` loop. The following `if` statement always executes:

```{r eval=FALSE}
if (A[i] is less than or equal to M) then
```

which requires $2$ instructions (one for the assignment and another for comparison). However the body of the `if` statement depends on the value $A[i]$. Therefore, it is tough to define a mathematical function. For instance, if $A = \{4,3,2,1\}$ then the `if` loop will be executed every time, costing us $2$ additional instructions (one for lookup and another for replacement/assignment), a total of 4 instructions, whereas if $A = \{5,9,13,20\}$ the body of the loop will not be executed even once, costing us only $2$ instructions. 

Often we will be interested in the worst-case because that is when the computation will be maximum. This is called the _worst case complexity_ - that is, it is the situation/complexity when we are highly unlucky. Thus, in the worst-case `if` is executed everytime costing us $4(n-1)$ for the entire `for` loop. Thus, the worst-case complexity function, $W(n)$, is given by,
$$W(n) = 4+2(n-1)+4(n-1) = 6n-2.$$
The best case is when the `if` loop body doesn't execute even once. Therefore, the best-case complexity, $B(n)$, is given by,
$$ B(n) = 4+2(n-1)+2(n-1) = 4n.$$
For a given, $n$, these functions, $W(n)$ and $B(n)$, will give us the number of instructions in the extreme cases.

---

#### An Introduction to AoA - Sorting


##### Selection Sort


##### Insertion Sort


##### Divide and Conquer 


---

#### Dynamic Programming 


##### Assembly Line Problem


##### Finding DNA Subsequence


---

#### Greedy Algorithms


---

#### An Introduction to NP-completeness


---

#### References

[A Gentle Introduction to Algorithm Complexity Analysis](http://discrete.gr/complexity/) by Dionysis "dionyziz" Zindros


