---
# title: "Lecture 02"
# author: "Kasthuri Kannan"
# date: "Sept 07, 2017"
output: html_document
# output: slidy_presentation

---

### Algorithms (Sept. 20, 2017)

---

This lecture is designed to give a bird's eye view of analysis of algorithms. The subject is extremely vast and it is usually a semester long course (typically a core course) in computer science departments. 

#### Motivation

Computer programs are set of instructions, or algorithm, supplied by the programmer to accomplish a certain task. There can be several algorithms that can accomplish the same computational task. For instance, consider sorting a bunch of cards. This could be done at least in two different ways, namely, by insertion sort or through selection sort -

---

<center>![](insertion_selection_sort.png)</center>

---

Which is a "better" sort, in terms of time, as the number of cards increase? The word "better" here is often referred as _computational complexity_ in the context of analysis of algorithms. 

Fundemental to analysis of algorithms is complexity analysis which helps in determining the computational complexity of such algorithms. The computational (time) complexity of an algorithm is something that decides how fast the algorithm is run at the idea level (otherwise known as pseudocode), regardless of the programming language, operating system or the hardware. We want to evaluate and compare algorithms as they are: an actionable/programmable idea on how things could be computed. In other words, we are **not interested** in how fast a program is actually run, which depends on various factors including the structure of high-level languages, but rather how best (or worst) is the design behind the program, regardless of the language it is implemented (by analogy, a person could sort faster or slower and we are not interested in the speed someone sorts). Such meta-analysis enables us to say an algorithm has a better computational complexity than the other.

Complexity analysis also allows us to explain how an algorithm would behave as the input size become large, or if the distribution of the input changes. For example, in the case of insertion or selection sort, we can _predict_ how the algorithm would perform if we give numbers that are already sorted vs. random unsorted numbers. Such predictions are important in practical programming since we can determine the scalability of operations assuming worst-case/best-case scenario.    

---

#### Counting instructions 

Computational complexity is usually determined by counting instructions. Let's find the complexity for the following computational task.

**Task** Detrmine the minimum element in an array $A$ with $n$ elements addressed by $A[1], A[2],\dots, A[n]$.

**Algorithm** The pesudocode to do this task is as follows:

```{r eval=FALSE}

  # Assign M to the first element in the array
  M = A[1]
  # Go through the array and if the number in the array is less than M, then update M with that number
  for i = 2 to n, increment i
    if (A[i] is less than or equal to M) then
        replace M with A[i]
    end if
  end for
  # Print the answer
  print M is the smallest number
    
```

To work out the complexity we will assume that our hypothetical machine can perform the following operations as one instruction each:

* Assigning a value to a variable
* Looking up the value of a particular element in an array
* Comparing two values
* Incrementing a value
* Basic arithmetic operations such as addition and multiplication

Thus, the first assignment statement:

```{r eval=FALSE}
  M = A[1]
```

will require $2$ instructions, one for looking up $A[1]$ and the other for assigning $A[1]$ to $M$. During the first time of the `for` loop, $i$ has to be assigned $2$ and the condition $2 \leq n$ needs to be evaluated, which will take two instuctions. After this, incrementing $i$ and checking if $i \leq n$ has to be done $n-1$ times, contributing $2(n-1)$ instructions.

We can now define a mathematical function $g(n)$ that, given an $n$, gives us the number of instructions the algorithm needs. Thus, if we ignore the `for` loop body (for the moment), the number of instructions would be $g(n)=2+2+2(n-1)=2n+2$.

---

#### Worst-case/Best-case complexity

Let's look into the `for` loop. The following `if` statement always executes:

```{r eval=FALSE}
  if (A[i] is less than or equal to M) then
```

which requires $2$ instructions (one for the assignment and another for comparison). However the body of the `if` statement depends on the value $A[i]$. Therefore, it is tough to define a mathematical function. For instance, if $A = \{4,3,2,1\}$ then the `if` loop will be executed every time, costing us $2$ additional instructions (one for lookup and another for replacement/assignment), a total of 4 instructions, whereas if $A = \{5,9,13,20\}$ the body of the loop will not be executed even once, costing us only $2$ instructions. 

Often we will be interested in the worst-case because that is when the computation will be maximum. This is called the _worst case complexity_ - that is, it is the situation/complexity when we are highly unlucky. Thus, in the worst-case `if` is executed everytime costing us $4(n-1)$ for the entire `for` loop. Thus, the worst-case complexity function, $W(n)$, is given by,
$$W(n) = 4+2(n-1)+4(n-1) = 6n-2.$$
The best case is when the `if` loop body doesn't execute even once. Therefore, the best-case complexity, $B(n)$, is given by,
$$ B(n) = 4+2(n-1)+2(n-1) = 4n.$$
For a given, $n$, these functions, $W(n)$ and $B(n)$ provides us a huristic on the number of instructions in the extreme cases, and gives an idea on how fast an algorithm is run as $n$ increases.

---

#### Asymptotic behavior

We should keep in mind that the **actual** number of CPU instructions would depend on the complier in our program of choice. 
For instance, the C compiler doesn't check if the specific array is within bounds, whereas a Pascal complier would. Therefore,
the assignment statement,

```{r eval=FALSE}
  M := A[i]
```

which would take just one instruction in C is equvilalent to the following statements in the Pascal language:

```{r eval=FALSE}
  if (i >= 0 && i < n) {
    M = A[i];
  }
```

It makes more sense to ignore the additive and multiplicative constants in our analysis and keep only the **rapidly growing term** that depends on the input size. Such an analysis is referred to as _asymptotic analysis_. So the asymptotic behavior of $W(n) = 6n-2$ is just $n$. Mathematically speaking, it just says that as $n$ increase, $W(n)$ is "as *bad* as $n$" and is denoted by $O(n)$.

Let us try to analyze the asymptotic behavior of the following task. 

**Task** Print the smallest number in a matrix $A(i,j)$ where $1 \leq i,j \leq n$, noting the column of the smallest element in each iteration, through _brute-force_ approach. 

Brute force approach is where we test all possible candidates for a solution and checking whether it solves our problem. Usually that's the simplest approach for solving a computational problem. The pseudocode for such algorithm is similar to the previous example except that we need to iterate through the columns as well. 

**Algorithm** Here is the pesudocode (the lines are numbered to make the analysis easier):

```{r eval=FALSE}

01.  # Assign M to the first element in the matrix
02.  M = A[1,1]
03.  smallest_element_column = 1
04.  # For every row i, iterate through all columns to see 
05.  # If the number A[i,j] is smaller than or equal to M
06.  for i = 1 to n, increment i
07.    for j = 1 to n, increment j
08.      if (A[i,j] is less than or equal to M) then
09.          # If the number A[i,j] is smaller than or equal to M, change M to A[i,j] 
10.          replace M with A[i,j]
11.          # Note the column where the smallest element in the iteration appears
12.          smallest_element_column = j
13.      end if
14.    end for       
15.    print "Column " smallest_element_column " has the smallest element"
16.  end for         
17.  # Print the answer
18.  print M is the smallest number
```

If we assume the above criteria for our hypothetical machine for per instruction cost, we note the following:

1. Lines 02 and 03 will count as $3$ instructions
2. In the worst case, Lines 10 and 12 will execute $n^2$ times each costing $5$ instructions ($2$ for the `if`, $2$ for Line 10 and $1$ for Line 12)
3. Line 15 will execute $n$ times and 
4. Line 18 will execute once costing $1$ instruction

giving us the worst case complexity, $$W(n) = 4+5n^2+n.$$ Hence, the aysmptotic behaviour, ignoring the constant factors and taking the rapidly growing term, is given by $O(n^2)$.

**Examples** 

1. $W(n) = 3n+2$ - asymptotic behavior is given by $O(n)$.
2. $W(n) = 217$  - asymptotic behavior is given by $O(1)$, since $217 = 1\times217$.
3. $W(n) = n + \sqrt{n}$ - asymptotic behavior is described by $O(n)$ since $n$ grows faster than $\sqrt{n}$.
4. $W(n) = n + e^{n}$ - asymptotic behavior is given by $O(e^{n})$ since $e^{n}$ rapidly grows than $n$, as $n$ increases.

**Exercises**

Find the asymptotic behavior of the following functions:

1. $W(n) = n^{3} + 3n$
2. $W(n) = 4^n + 1047$
3. $W(n) = 4^n + n^4$
4. $W(n) = n^n + n$

---

#### The complexity of insertion sort and selection sort

<br>

##### Insertion sort

<br>

<iframe width="560" height="315" src="https://www.youtube.com/embed/DFG-XuyPYUQ" frameborder="0" allowfullscreen></iframe>

---

#### Divide and Conquer 


---

#### Dynamic Programming 


##### Assembly Line Problem


##### Finding DNA Subsequence


---

#### Greedy Algorithms


---

#### An Introduction to NP-completeness


---

#### References

[A Gentle Introduction to Algorithm Complexity Analysis](http://discrete.gr/complexity/) by Dionysis "dionyziz" Zindros


