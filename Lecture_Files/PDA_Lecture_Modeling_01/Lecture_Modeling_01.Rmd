---
# title: "Lecture 03"
# author: "Kasthuri Kannan"
# date: "Sept 07, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
body {
text-align: justify}
</style>

---

### Modeling fundamentals 01: Basic inference and linear modeling (Oct. 31, 2017)

---

#### Basic inference

We will first study basic ideas in statistical inference using our HANES data set.


```{r eval=TRUE, message=FALSE}
  # Load the package RCurl
  library(RCurl)
  # Import the HANES data set from GitHub; break the string into two for readability
  # (Please note this readability aspect very carefully)
  URL_text_1 <- "https://raw.githubusercontent.com/kannan-kasthuri/kannan-kasthuri.github.io"
  URL_text_2 <- "/master/Datasets/HANES/NYC_HANES_DIAB.csv"
  # Paste it to constitute a single URL 
  URL <- paste(URL_text_1,URL_text_2, sep="")
  HANES <- read.csv(text=getURL(URL))
  # Rename the GENDER factor for identification
  HANES$GENDER <- factor(HANES$GENDER, labels=c("M","F"))
  # Rename the AGEGROUP factor for identification
  HANES$AGEGROUP <- factor(HANES$AGEGROUP, labels=c("20-39","40-59","60+"))
  # Rename the HSQ_1 factor for identification
  HANES$HSQ_1 <- factor(HANES$HSQ_1, labels=c("Excellent","Very Good","Good", "Fair", "Poor"))
  # Rename the DX_DBTS as a factor
  HANES$DX_DBTS <- factor(HANES$DX_DBTS, labels=c("DIAB","DIAB NO_DX","NO DIAB"))
  # Omit all NA from the data frame
  HANES <- na.omit(HANES)
  # Observe the structure
  str(HANES)
  # Load the tidyverse library
  library(tidyverse)
```

Also, we need a package "rafalib" for these materials. So, lets go ahead and install that package, and load the library.

```{r eval=FALSE, message=FALSE, warning=TRUE, echo=TRUE}
  # Install rafaliab library
  install.packages("rafalib")
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=TRUE}
  # Load rafalib library
  library(rafalib)
```


##### Z-Test

According to central limit theorem, if $V$ and $Z$ are independent and identically distributed random variables that represents a population, with mean $\mu_{V}$ and $\mu_{Z}$, and standard deviation, $\sigma_{V}$ and $\sigma_{Z}$, respectively, then the ratio,

$$
R = \frac{\mu_{Z}-\mu_{V}}{\sqrt{\frac{\sigma_Z^2}{N} + \frac{\sigma_V^2}{N}}}
$$
approaches normal distribution centered at $0$ and standard deviation $1$ as $N$ becomes large. In general, we will not know this mean and standard deviation for the population but only for a random sample $X$ and $Y$ from the general population. In case $V$ and $Z$ are normally distributed, the approximation to the mean $\mu_{V}$ and $\mu_{Z}$ given by $\hat{X}$ and $\hat{Y}$ are also normally distributed, and so are their difference. Therefore, the ratio $\hat{R}$, described by,

$$
\hat{R} = \frac{\hat{Y}-\hat{X}}{\sqrt{\frac{\sigma_X^2}{N} + \frac{\sigma_Y^2}{N}}}
$$

can be used to compute p-values simply because we know the proportion of the distribution under any value. For example, only 7% of these values are larger than 1.8 (in absolute value)

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  pnorm(-1.8) + (1 - pnorm(1.8))
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  pnorm(-1.8) + (1 - pnorm(1.8))
```

Thus, if we know:

1. the population is normally distributed,
2. the standard deviation of the population and 
3. the sample size is large,

we can use this approximation. This is known as $Z$-test.

Most of the times, these conditions are hardly met. For the first condition that the population being normally distributed, although in practice we may not know the population distribution, we can see how close the sample distribution is close to normal distribution by plotting _qq-plots_ in order to confirm that the distributions are relatively close to being normally distributed.

We will work with the blood hemoglobin variable, A1C. We will consider people without diabetes as control group, that represents the (sample) random variable $Y$ and people with diabetes as disease group - that represents the (sample) random variable $X$. 

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Extract only the A1C variable for the control group 
  control <- filter(HANES, DX_DBTS == "NO DIAB") %>% select(A1C)
  # and the disease group
  disease <- filter(HANES, DX_DBTS == "DIAB") %>% select(A1C)
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Extract only the A1C variable for the control group 
  control <- filter(HANES, DX_DBTS == "NO DIAB") %>% select(A1C)
  # and the disease group
  disease <- filter(HANES, DX_DBTS == "DIAB") %>% select(A1C)
```

We can now make a qqplot for the control variable

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Find the 1st and 3rd quartiles
  y <- quantile(unlist(control), c(0.25, 0.75))
  # Find the matching normal values on the x-axis
  x <- qnorm( c(0.25, 0.75))
  # Compute the line slope
  slope <- diff(y) / diff(x) 
  # Compute the line intercept
  int <- y[1] - slope * x[1]
  # Generate normal q-q plot
  ggplot() + aes(sample=unlist(control)) + stat_qq(distribution=qnorm) + 
        geom_abline(intercept=int, slope=slope) + ylab("Sample") +
        labs(title = "QQ Plot for Control Variable")
```


```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Find the 1st and 3rd quartiles
  y <- quantile(unlist(control), c(0.25, 0.75))
  # Find the matching normal values on the x-axis
  x <- qnorm( c(0.25, 0.75))
  # Compute the line slope
  slope <- diff(y) / diff(x) 
  # Compute the line intercept
  int <- y[1] - slope * x[1]
  # Generate normal q-q plot
  ggplot() + aes(sample=unlist(control)) + stat_qq(distribution=qnorm) + 
        geom_abline(intercept=int, slope=slope) + ylab("Sample") +
        labs(title = "QQ Plot for Control Variable")
```

Similarly we can make a qqplot for the disease variable.

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Find the 1st and 3rd quartiles
  y <- quantile(unlist(disease), c(0.25, 0.75))
  # Find the matching normal values on the x-axis
  x <- qnorm( c(0.25, 0.75))
  # Compute the line slope
  slope <- diff(y) / diff(x) 
  # Compute the line intercept
  int <- y[1] - slope * x[1]
  # Generate normal q-q plot
  ggplot() + aes(sample=unlist(disease)) + stat_qq(distribution=qnorm) + 
        geom_abline(intercept=int, slope=slope) + ylab("Sample") +
        labs(title = "QQ Plot for Disease Variable")
```


```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Find the 1st and 3rd quartiles
  y <- quantile(unlist(disease), c(0.25, 0.75))
  # Find the matching normal values on the x-axis
  x <- qnorm( c(0.25, 0.75))
  # Compute the line slope
  slope <- diff(y) / diff(x) 
  # Compute the line intercept
  int <- y[1] - slope * x[1]
  # Generate normal q-q plot
  ggplot() + aes(sample=unlist(disease)) + stat_qq(distribution=qnorm) + 
        geom_abline(intercept=int, slope=slope) + ylab("Sample") +
        labs(title = "QQ Plot for Disease Variable")
```

We note that the control variable is more normal-like than the disease variable. Thus, a $Z$-test may not be appropriate. Even if the disease variable is normally distributed, a major disadvantage of a $Z$-test is that it requires the standard deviation of the population which we will not know in reality.

<br>
<span style="color:blue">**Classwork/Homework**</span>: Check if the GLUCOSE values for the _non-diabetic_ male and female population are close to normal by constructing qqplots.

<br>

##### t-Test

Even if we don't know the population standard deviations, we can estimate from the sample standard deviations:

$$ s_X^2 = \frac{1}{N-1} \sum_{i=1}^N (x_i - \hat{X})^2 \mbox{ and } s_Y^2 = \frac{1}{N-1} \sum_{i=1}^N (y_i - \hat{Y})^2 $$

to calculate the distribution of:

$$ t = \sqrt{N} \frac{\hat{Y}-\hat{X}}{\sqrt{s_{X}^{2}+s_{Y}^{2}}} $$
which is _approximately_ normal. This is called the $t$-statistic and the distribution is called _Student's t-distribution_. 

Although the disease variable in our case is not really normally distributed, _had it been_ normally distributed, we can apply the t-test to find the p-value:

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=TRUE}
  # Apply t-test to disease vs. control population
  t.test(disease,control)
```


<br>
<span style="color:blue">**Classwork/Homework**</span>: Perform t-test between male and female populations in non-diabetic sub population based on the GLUCOSE variable.

<br>

##### Confidence intervals

p-values are everywhere in life sciences.But it is not good to report only p-values for the simple reason that statistical significance doesn't mean causality or scientific significance. A better alternative is to report confidence interval. Confidence intervals include information about the uncertainty associated with this estimate. 

A 95% confidence interval (we can use percentages other than 95%) is a random interval with a 95% probability of falling on the parameter we are estimating. To construct it, we note that the central limit theorem tells us that $\sqrt{N} (\bar{X}-\mu_X) / s_X$ follows a normal distribution with mean 0 and standard deviation 1. This implies that the probability of this event is (where $p$ is the 95% intreval around 0):

$$ -p \leq \sqrt{N} (\bar{X}-\mu_X)/s_X \leq p $$

is about 95% because:

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=TRUE}
  # Calculate the probability of the above variable
  pnorm(2) - pnorm(-2)
```

Now doing some algebra on the above inequality we see that:

$$ \bar{X}-p* s_X/\sqrt{N} \leq \mu_X \leq \bar{X}+p*s_X/\sqrt{N} $$

the chance of mean falling into the interval has a probability of 95%. Thus, we can compute the above estimate:

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Find the estimate of s_{X}/sqrt(N)
  se <- sd(disease$A1C)/sqrt(length(disease$A1C))
  # Find the 95% semi-interval around the normal distribution
  p <- qnorm(1- 0.05/2)
  # Compute the confidence interval
  interval <- c(mean(disease$A1C)-p*se, mean(disease$A1C)+p*se)
  # Check if the mean of the disease population is in the confidence interval 
  interval[1] < mean(disease$A1C) & interval[2] > mean(disease$A1C)
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Find the estimate of s_{X}/sqrt(N)
  se <- sd(disease$A1C)/sqrt(length(disease$A1C))
  # Find the 95% semi-interval around the normal distribution
  p <- qnorm(1- 0.05/2)
  # Compute the confidence interval
  interval <- c(mean(disease$A1C)-p*se, mean(disease$A1C)+p*se)
  # Check if the mean of the disease population is in the confidence interval 
  interval[1] < mean(disease$A1C) & interval[2] > mean(disease$A1C)
```


<br>
<span style="color:blue">**Classwork/Homework**</span>: Report the confidence interval for the mean GLUCOSE variable for both female and male population for the non-diabetic sub population. Do they overlap?

<br>


##### Association tests

Several life science and biomedical data sets come not just with continuous variables, but with categorical, binary and ordinal variables. For example, a locus in the genome has a mutation or not, is a binary variable on a set of loci. Association tests are extremely useful in detrmining whether such variables exhibit association between the categories. For example, lets consider the MIMIC3 admissions data:

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Load the package RCurl
  library(RCurl)
  # Import the admissions data set in MIMIC3 from GitHub; 
  URL_text_1 <- "https://raw.githubusercontent.com/kannan-kasthuri/kannan-kasthuri.github.io"
  URL_text_2 <- "/master/Datasets/MIMIC3/admissions.csv"
  URL <- paste(URL_text_1,URL_text_2, sep="")
  MIMIC3_ADM <- read.csv(text=getURL(URL))
  # Observe the structure
  str(MIMIC3_ADM)
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Load the package RCurl
  library(RCurl)
  # Import the admissions data set in MIMIC3 from GitHub; 
  URL_text_1 <- "https://raw.githubusercontent.com/kannan-kasthuri/kannan-kasthuri.github.io"
  URL_text_2 <- "/master/Datasets/MIMIC3/admissions.csv"
  URL <- paste(URL_text_1,URL_text_2, sep="")
  MIMIC3_ADM <- read.csv(text=getURL(URL))
  # Observe the structure
  str(MIMIC3_ADM)
```

Let us find the number of people who are separated, have Medicaid and Government insurance and find their ethinicity.

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  
  # Find the count of insurance and marital status
  insurance_marital.status <- MIMIC3_ADM %>% 
    group_by(insurance, marital_status) %>% count()
  insurance_marital.status
  # Find the number of people with medicaid
  folks_with_medicaid <- MIMIC3_ADM %>% 
    filter(insurance == "Medicaid") %>% count() 
  folks_with_medicaid$n
  # Find the ethinicities for the separated Medicaid group
  MIMIC3_ADM %>% 
    filter(insurance =="Medicaid" & marital_status == "SEPARATED") %>% 
    group_by(ethnicity) %>% count()
  # Find the ethinicities for the separated Government group
  MIMIC3_ADM %>% 
    filter(insurance =="Government" & marital_status == "SEPARATED") %>% 
    group_by(ethnicity) %>% count()
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  
  # Find the count of insurance and marital status
  insurance_marital.status <- MIMIC3_ADM %>% 
    group_by(insurance, marital_status) %>% count()
  insurance_marital.status
  # Find the number of people with medicaid
  folks_with_medicaid <- MIMIC3_ADM %>% 
    filter(insurance == "Medicaid") %>% count() 
  folks_with_medicaid$n
  # Find the ethinicities for the separated Medicaid group
  MIMIC3_ADM %>% 
    filter(insurance =="Medicaid" & marital_status == "SEPARATED") %>% 
    group_by(ethnicity) %>% count()
  # Find the ethinicities for the separated Government group
  MIMIC3_ADM %>% 
    filter(insurance =="Government" & marital_status == "SEPARATED") %>% 
    group_by(ethnicity) %>% count()
```

The question we ask about the data is the following: knowing that $3$ out of $5$ separated blacks have Medicaid insurance, and $2$ out of $5$ have Government insurance, can we REJECT the null hypothesis ($0.05$ level) that the odds of finding separated blacks with Medicaid is the same as the odds of finding them with Government insurance? To answer this question we can perform Fisher's exact test:

**Fisher's exact test**

We set up the contingency table as follows:

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Make a data table and define row names and column names
  tab <- matrix(c(3,2,9,1),2,2)
  colnames(tab)<-c("BLACK","NON-BLACK")
  rownames(tab)<-c("MEDICAID","GOVERNMENT")
  tab
  # Run Fisher's exact test
  fisher.test(tab)
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Make a data table and define row names and column names
  tab <- matrix(c(3,2,9,1),2,2)
  colnames(tab)<-c("BLACK","NON-BLACK")
  rownames(tab)<-c("MEDICAID","GOVERNMENT")
  tab
  # Run Fisher's exact test
  fisher.test(tab)
```

The p-value of $0.24$ suggests that null hypothesis holds and that the odds of finding separated black people with Medicaid are almost the same with Government insurance.

**Chi-square test**

In the above example we had a sample size of $15$. If the sample sizes are large, chi-square test gives a good approximation for the confidence intervals in hypothesis testing that will allow us to determine association. Let's use chi-square test to see if there is an association between ethnicity and dignosis of a disease. We will consider two ethnicities - WHITE and BLACK and the diagnosis of DIABETIC KETOACIDOSIS.   

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  
  # Find the diagnosis and ethnicity info by grouping 
  diagnosis_ethnicity <- MIMIC3_ADM %>% 
    group_by(diagnosis, ethnicity) %>% count() %>% arrange(desc(n))
  # Display the table
  diagnosis_ethnicity
  # Select only the required ethinicity
  MIMIC3_ADM %>% filter((ethnicity == "BLACK/AFRICAN AMERICAN" | ethnicity == "WHITE") & diagnosis == "DIABETIC KETOACIDOSIS") %>% group_by(ethnicity,diagnosis) %>% count()
  # Find the count of the population
  black_pop <- MIMIC3_ADM %>% filter(ethnicity == "BLACK/AFRICAN AMERICAN") %>% count()
  white_pop <- MIMIC3_ADM %>% filter(ethnicity == "WHITE") %>% count()
  black_pop$n
  white_pop$n
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  
  # Find the diagnosis and ethnicity info by grouping 
  diagnosis_ethnicity <- MIMIC3_ADM %>% 
    group_by(diagnosis, ethnicity) %>% count() %>% arrange(desc(n))
  # Display the table
  diagnosis_ethnicity
  # Select only the required ethinicity
  MIMIC3_ADM %>% filter((ethnicity == "BLACK/AFRICAN AMERICAN" | ethnicity == "WHITE") & diagnosis == "DIABETIC KETOACIDOSIS") %>% group_by(ethnicity,diagnosis) %>% count()
  # Find the count of the population
  black_pop <- MIMIC3_ADM %>% filter(ethnicity == "BLACK/AFRICAN AMERICAN") %>% count()
  white_pop <- MIMIC3_ADM %>% filter(ethnicity == "WHITE") %>% count()
  black_pop$n
  white_pop$n
```

From the above analysis we see that $33$ and $18$ patients are affected by DIABETIC KETOACIDOSIS from the black and white ethnicities. Also, from the total count we have `r black_pop$n-33` and `r white_pop$n-18` are unaffected by the disease. This information would allow us to perform the chi-square test by forming a contingency table:

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Make a data table and define row names and column names
  tab <- matrix(c(510,33,3982,18),2,2)
  colnames(tab)<-c("BLACK","WHITE")
  rownames(tab)<-c("Unaffected","DIAB.KET")
  tab
  # Run chi-square test
  chisq.test(tab)
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Make a data table and define row names and column names
  tab <- matrix(c(510,33,3982,18),2,2)
  colnames(tab)<-c("BLACK","WHITE")
  rownames(tab)<-c("Unaffected","DIAB.KET")
  tab
  # Run chi-square test
  chisq.test(tab)
```

Thus, from the small p-value we infer that there is a good association between DIABETIC KETOACIDOSIS and the black population. Aparantly, it appears that this disease is common among the black and hispanic populations  as one can see from the [reference](http://annals.org/aim/article/720954/narrative-review-ketosis-prone-type-2-diabetes-mellitus).

<br>
<span style="color:blue">**Classwork/Homework**</span>: Perform chi-square test and report association (or not) between the variables of your choice in your favorite data set.

<br>

---

#### Selected materials and references

[R for Data Science - Introduction to Modeling](http://r4ds.had.co.nz/model-basics.html)

[PH525x series Biomedical Data Science](http://genomicsclass.github.io/book/)

---