---
# title: "Lecture 03"
# author: "Kasthuri Kannan"
# date: "Sept 07, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
body {
text-align: justify}
</style>

---

### Modeling fundamentals 01: Basic inference and linear modeling (Oct. 31, 2017)

---

#### Basic inference

We will first study basic ideas in statistical inference using our HANES data set.


```{r eval=TRUE, message=FALSE}
  # Load the package RCurl
  library(RCurl)
  # Import the HANES data set from GitHub; break the string into two for readability
  # (Please note this readability aspect very carefully)
  URL_text_1 <- "https://raw.githubusercontent.com/kannan-kasthuri/kannan-kasthuri.github.io"
  URL_text_2 <- "/master/Datasets/HANES/NYC_HANES_DIAB.csv"
  # Paste it to constitute a single URL 
  URL <- paste(URL_text_1,URL_text_2, sep="")
  HANES <- read.csv(text=getURL(URL))
  # Rename the GENDER factor for identification
  HANES$GENDER <- factor(HANES$GENDER, labels=c("M","F"))
  # Rename the AGEGROUP factor for identification
  HANES$AGEGROUP <- factor(HANES$AGEGROUP, labels=c("20-39","40-59","60+"))
  # Rename the HSQ_1 factor for identification
  HANES$HSQ_1 <- factor(HANES$HSQ_1, labels=c("Excellent","Very Good","Good", "Fair", "Poor"))
  # Rename the DX_DBTS as a factor
  HANES$DX_DBTS <- factor(HANES$DX_DBTS, labels=c("DIAB","DIAB NO_DX","NO DIAB"))
  # Omit all NA from the data frame
  HANES <- na.omit(HANES)
  # Observe the structure
  str(HANES)
  # Load the tidyverse library
  library(tidyverse)
```

Also, we need a package "rafalib" for these materials. So, lets go ahead and install that package, and load the library.

```{r eval=FALSE, message=FALSE, warning=TRUE, echo=TRUE}
  # Install rafaliab library
  install.packages("rafalib")
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=TRUE}
  # Load rafalib library
  library(rafalib)
```


##### Z-Test

According to central limit theorem, if $V$ and $Z$ are independent and identically distributed random variables that represents a population, with mean $\mu_{V}$ and $\mu_{Z}$, and standard deviation, $\sigma_{V}$ and $\sigma_{Z}$, respectively, then the ratio,

$$
R = \frac{\mu_{Z}-\mu_{V}}{\sqrt{\frac{\sigma_Z^2}{N} + \frac{\sigma_V^2}{N}}}
$$
approaches normal distribution centered at $0$ and standard deviation $1$ as $N$ becomes large. In general, we will not know this mean and standard deviation for the population but only for a random sample $X$ and $Y$ from the general population. In case $V$ and $Z$ are normally distributed, the approximation to the mean $\mu_{V}$ and $\mu_{Z}$ given by $\hat{X}$ and $\hat{Y}$ are also normally distributed, and so are their difference. Therefore, the ratio $\hat{R}$, described by,

$$
\hat{R} = \frac{\hat{Y}-\hat{X}}{\sqrt{\frac{\sigma_X^2}{N} + \frac{\sigma_Y^2}{N}}}
$$

can be used to compute p-values simply because we know the proportion of the distribution under any value. For example, only 7% of these values are larger than 1.8 (in absolute value)

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  pnorm(-1.8) + (1 - pnorm(1.8))
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  pnorm(-1.8) + (1 - pnorm(1.8))
```

Thus, if we know:

1. the population is normally distributed,
2. the standard deviation of the population and 
3. the sample size is large,

we can use this approximation. This is known as $Z$-test.

Most of the times, these conditions are hardly met. For the first condition that the population being normally distributed, although in practice we may not know the population distribution, we can see how close the sample distribution is close to normal distribution by plotting _qq-plots_ in order to confirm that the distributions are relatively close to being normally distributed.

We will work with the blood hemoglobin variable, A1C. We will consider people without diabetes as control group, that represents the (sample) random variable $Y$ and people with diabetes as disease group - that represents the (sample) random variable $X$. 

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Extract only the A1C variable for the control group 
  control <- filter(HANES, DX_DBTS == "NO DIAB") %>% select(A1C)
  # and the disease group
  disease <- filter(HANES, DX_DBTS == "DIAB") %>% select(A1C)
```

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Extract only the A1C variable for the control group 
  control <- filter(HANES, DX_DBTS == "NO DIAB") %>% select(A1C)
  # and the disease group
  disease <- filter(HANES, DX_DBTS == "DIAB") %>% select(A1C)
```

We can now make a qqplot for the control variable

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Find the 1st and 3rd quartiles
  y <- quantile(unlist(control), c(0.25, 0.75))
  # Find the matching normal values on the x-axis
  x <- qnorm( c(0.25, 0.75))
  # Compute the line slope
  slope <- diff(y) / diff(x) 
  # Compute the line intercept
  int <- y[1] - slope * x[1]
  # Generate normal q-q plot
  ggplot() + aes(sample=unlist(control)) + stat_qq(distribution=qnorm) + 
        geom_abline(intercept=int, slope=slope) + ylab("Sample") +
        labs(title = "QQ Plot for Control Variable")
```


```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Find the 1st and 3rd quartiles
  y <- quantile(unlist(control), c(0.25, 0.75))
  # Find the matching normal values on the x-axis
  x <- qnorm( c(0.25, 0.75))
  # Compute the line slope
  slope <- diff(y) / diff(x) 
  # Compute the line intercept
  int <- y[1] - slope * x[1]
  # Generate normal q-q plot
  ggplot() + aes(sample=unlist(control)) + stat_qq(distribution=qnorm) + 
        geom_abline(intercept=int, slope=slope) + ylab("Sample") +
        labs(title = "QQ Plot for Control Variable")
```

Similarly we can make a qqplot for the disease variable.

```{r eval=FALSE, message=FALSE, warning=FALSE, echo=TRUE, na.rm=FALSE}
  # Find the 1st and 3rd quartiles
  y <- quantile(unlist(disease), c(0.25, 0.75))
  # Find the matching normal values on the x-axis
  x <- qnorm( c(0.25, 0.75))
  # Compute the line slope
  slope <- diff(y) / diff(x) 
  # Compute the line intercept
  int <- y[1] - slope * x[1]
  # Generate normal q-q plot
  ggplot() + aes(sample=unlist(disease)) + stat_qq(distribution=qnorm) + 
        geom_abline(intercept=int, slope=slope) + ylab("Sample") +
        labs(title = "QQ Plot for Disease Variable")
```


```{r eval=TRUE, message=FALSE, warning=TRUE, echo=FALSE}
  # Find the 1st and 3rd quartiles
  y <- quantile(unlist(disease), c(0.25, 0.75))
  # Find the matching normal values on the x-axis
  x <- qnorm( c(0.25, 0.75))
  # Compute the line slope
  slope <- diff(y) / diff(x) 
  # Compute the line intercept
  int <- y[1] - slope * x[1]
  # Generate normal q-q plot
  ggplot() + aes(sample=unlist(disease)) + stat_qq(distribution=qnorm) + 
        geom_abline(intercept=int, slope=slope) + ylab("Sample") +
        labs(title = "QQ Plot for Disease Variable")
```

We note that the control variable is more normal-like than the disease variable. Thus, a $Z$-test may not be appropriate. Even if the disease variable is normally distributed, a major disadvantage of a $Z$-test is that it requires the standard deviation of the population which we will not know in reality.

##### t-Test

Even if we don't know the population standard deviations, we can estimate from the sample standard deviations:

$$ s_X^2 = \frac{1}{N-1} \sum_{i=1}^N (x_i - \hat{X})^2 \mbox{ and } s_Y^2 = \frac{1}{N-1} \sum_{i=1}^N (y_i - \hat{Y})^2 $$

to calculate the distribution of:

$$ t = \sqrt{N} \frac{\hat{Y}-\hat{X}}{\sqrt{s_{X}^{2}+s_{Y}^{2}}} $$
which is _approximately_ normal. This is called the $t$-statistic and the distribution is called _Student's t-distribution_. 

Although the disease variable in our case is not really normally distributed, _had it been_ normally distributed, we can apply the t-test to find the p-value:

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=TRUE}
  # Apply t-test to disease vs. control population
  t.test(disease,control)
```

##### Confidence intervals

p-values are everywhere in life sciences.But it is not good to report only p-values for the simple reason that statistical significance doesn't mean causality or scientific significance. A better alternative is to report confidence interval. Confidence intervals include information about the uncertainty associated with this estimate. 

A 95% confidence interval (we can use percentages other than 95%) is a random interval with a 95% probability of falling on the parameter we are estimating. To construct it, we note that the central limit theorem tells us that $\sqrt{N} (\bar{X}-\mu_X) / s_X$ follows a normal distribution with mean 0 and standard deviation 1. This implies that the probability of this event:

$$ -2 \leq \sqrt{N} (\bar{X}-\mu_X)/s_X \leq 2 $$

is about 95% because:

```{r eval=TRUE, message=FALSE, warning=TRUE, echo=TRUE}
  # Calculate the probability of the above variable
  pnorm(2) - pnorm(-2)
```

Now doing some algebra on the above inequality we see that:

$$ \bar{X}-2 s_X/\sqrt{N} \leq \mu_X \leq \bar{X}+2s_X/\sqrt{N} $$

the chance of mean falling into the interval has a probability of 95%. Thus, we can compute:

---

#### Selected materials and references

[R for Data Science - Introduction to Modeling](http://r4ds.had.co.nz/model-basics.html)

[PH525x series Biomedical Data Science](http://genomicsclass.github.io/book/)

---